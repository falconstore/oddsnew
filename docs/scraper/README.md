# ğŸ¯ Odds Scraper - Sistema de Coleta de Odds

Sistema Python para coleta automatizada de odds de apostas esportivas de mÃºltiplas casas de apostas.

## ğŸ“ Estrutura do Projeto

```
odds-scraper/
â”œâ”€â”€ .env                    # VariÃ¡veis de ambiente (NÃƒO versionar!)
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ config.py              # ConfiguraÃ§Ãµes e constantes
â”œâ”€â”€ main.py                # Entry point da aplicaÃ§Ã£o
â”œâ”€â”€ orchestrator.py        # Gerenciador de execuÃ§Ã£o paralela
â”œâ”€â”€ base_scraper.py        # Classe base para scrapers
â”œâ”€â”€ supabase_client.py     # Cliente de banco de dados
â”œâ”€â”€ team_matcher.py        # Fuzzy matching de nomes
â””â”€â”€ scrapers/              # ImplementaÃ§Ãµes especÃ­ficas
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ betano.py
    â”œâ”€â”€ bet365.py
    â”œâ”€â”€ sportingbet.py
    â”œâ”€â”€ betfair.py
    â””â”€â”€ onexbet.py
```

## ğŸš€ InstalaÃ§Ã£o

### 1. Requisitos do Sistema

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install -y python3.10 python3-pip chromium-browser

# Verificar versÃ£o do Python
python3 --version  # Deve ser 3.10+
```

### 2. Configurar Ambiente Virtual

```bash
# Criar e ativar ambiente virtual
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install -r requirements.txt

# Instalar navegador do Playwright
playwright install chromium
```

### 3. Configurar VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# Supabase (obtenha em: Settings > API no Supabase Dashboard)
SUPABASE_URL=https://seu-projeto.supabase.co
SUPABASE_SERVICE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6...

# ConfiguraÃ§Ãµes de Scraping
SCRAPE_INTERVAL_SECONDS=60
STALE_DATA_THRESHOLD_MINUTES=30

# Logging
LOG_LEVEL=INFO
LOG_FILE=scraper.log

# Alertas
VALUE_BET_THRESHOLD=5.0
ARBITRAGE_THRESHOLD=0.0
```

âš ï¸ **IMPORTANTE**: Use a `service_role` key (nÃ£o a `anon` key) para o scraper ter acesso completo Ã s tabelas.

## ğŸƒ Executar

### Modo Desenvolvimento (uma execuÃ§Ã£o)

```bash
python main.py --once --debug
```

### Modo ProduÃ§Ã£o (loop contÃ­nuo)

```bash
python main.py
```

### Com Intervalo Customizado

```bash
python main.py --interval 120  # A cada 2 minutos
```

### Em Background (nohup)

```bash
nohup python main.py > /dev/null 2>&1 &
```

### Como ServiÃ§o (systemd)

Crie `/etc/systemd/system/odds-scraper.service`:

```ini
[Unit]
Description=Odds Scraper Service
After=network.target

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/home/ubuntu/odds-scraper
Environment=PATH=/home/ubuntu/odds-scraper/venv/bin
ExecStart=/home/ubuntu/odds-scraper/venv/bin/python main.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl enable odds-scraper
sudo systemctl start odds-scraper
sudo systemctl status odds-scraper
```

## ğŸ”§ Implementar um Novo Scraper

### 1. Criar arquivo do scraper

```python
# scrapers/betano.py
from playwright.async_api import async_playwright
from base_scraper import BaseScraper, ScrapedOdds, LeagueConfig
from config import BOOKMAKERS, LEAGUE_CONFIGS

class BetanoScraper(BaseScraper):
    def __init__(self):
        config = BOOKMAKERS["betano"]
        super().__init__(config["name"], config["base_url"])
        self._browser = None
        self._page = None
    
    async def setup(self):
        """Inicializa o browser."""
        self._playwright = await async_playwright().start()
        self._browser = await self._playwright.chromium.launch(headless=True)
        self._page = await self._browser.new_page()
    
    async def teardown(self):
        """Fecha o browser."""
        if self._browser:
            await self._browser.close()
        if self._playwright:
            await self._playwright.stop()
    
    async def get_available_leagues(self) -> list[LeagueConfig]:
        """Retorna ligas configuradas para Betano."""
        leagues = []
        for league_id, config in LEAGUE_CONFIGS.items():
            if "betano" in config["urls"]:
                leagues.append(LeagueConfig(
                    league_id=league_id,
                    name=config["name"],
                    url=self.base_url + config["urls"]["betano"],
                    country=config["country"]
                ))
        return leagues
    
    async def scrape_league(self, league: LeagueConfig) -> list[ScrapedOdds]:
        """Coleta odds de uma liga especÃ­fica."""
        odds_list = []
        
        await self._page.goto(league.url, wait_until="networkidle")
        await self._page.wait_for_selector('[data-qa="event-row"]', timeout=10000)
        
        events = await self._page.query_selector_all('[data-qa="event-row"]')
        
        for event in events:
            try:
                # Extrair dados (ajustar seletores conforme o site)
                home_el = await event.query_selector('[data-qa="home-team"]')
                away_el = await event.query_selector('[data-qa="away-team"]')
                odds_els = await event.query_selector_all('[data-qa="odd-value"]')
                date_el = await event.query_selector('[data-qa="event-date"]')
                
                if home_el and away_el and len(odds_els) >= 3:
                    home_team = await home_el.inner_text()
                    away_team = await away_el.inner_text()
                    date_text = await date_el.inner_text() if date_el else ""
                    
                    odds_list.append(ScrapedOdds(
                        bookmaker_name=self.name,
                        home_team_raw=home_team.strip(),
                        away_team_raw=away_team.strip(),
                        league_raw=league.name,
                        match_date=self._parse_date(date_text),
                        home_odd=self._parse_odds(await odds_els[0].inner_text()),
                        draw_odd=self._parse_odds(await odds_els[1].inner_text()),
                        away_odd=self._parse_odds(await odds_els[2].inner_text()),
                    ))
            except Exception as e:
                self.logger.warning(f"Erro ao processar evento: {e}")
        
        return odds_list
```

### 2. Registrar no Orchestrator

```python
# main.py
from scrapers.betano import BetanoScraper

def create_orchestrator() -> Orchestrator:
    orchestrator = Orchestrator()
    orchestrator.register_scraper(BetanoScraper())
    # ... outros scrapers
    return orchestrator
```

## ğŸ“Š Fluxo de Dados

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VPS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  main.py                                                    â”‚
â”‚     â”‚                                                       â”‚
â”‚     â–¼                                                       â”‚
â”‚  Orchestrator                                               â”‚
â”‚     â”‚                                                       â”‚
â”‚     â”œâ”€â”€â–º BetanoScraper â”€â”€â–º HTML â”€â”€â–º ScrapedOdds            â”‚
â”‚     â”œâ”€â”€â–º Bet365Scraper â”€â”€â–º HTML â”€â”€â–º ScrapedOdds            â”‚
â”‚     â””â”€â”€â–º SportingbetScraper â”€â”€â–º HTML â”€â”€â–º ScrapedOdds       â”‚
â”‚              â”‚                                              â”‚
â”‚              â–¼                                              â”‚
â”‚         TeamMatcher (fuzzy matching)                        â”‚
â”‚              â”‚                                              â”‚
â”‚              â–¼                                              â”‚
â”‚         NormalizaÃ§Ã£o (team_id, league_id, match_id)        â”‚
â”‚              â”‚                                              â”‚
â”‚              â–¼                                              â”‚
â”‚         AlertDetector (arbitragem, value bets)             â”‚
â”‚              â”‚                                              â”‚
â”‚              â–¼                                              â”‚
â”‚         SupabaseClient.insert_odds()                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Supabase                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  odds_history, matches, teams, team_aliases, alerts        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Lovable Dashboard                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  React App â”€â”€â–º Supabase Client â”€â”€â–º VisualizaÃ§Ã£o            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” Debugging

### Ver logs em tempo real

```bash
tail -f scraper.log
```

### Testar um scraper individualmente

```python
# test_scraper.py
import asyncio
from scrapers.betano import BetanoScraper

async def test():
    scraper = BetanoScraper()
    await scraper.setup()
    
    leagues = await scraper.get_available_leagues()
    print(f"Ligas disponÃ­veis: {len(leagues)}")
    
    for league in leagues[:1]:  # Testar apenas primeira liga
        odds = await scraper.scrape_league(league)
        print(f"{league.name}: {len(odds)} jogos")
        for o in odds[:3]:
            print(f"  {o.home_team_raw} vs {o.away_team_raw}: {o.home_odd}/{o.draw_odd}/{o.away_odd}")
    
    await scraper.teardown()

asyncio.run(test())
```

## âš ï¸ ConsideraÃ§Ãµes Importantes

1. **Rate Limiting**: Adicione delays entre requisiÃ§Ãµes para evitar bloqueios
2. **Proxies**: Para produÃ§Ã£o, considere usar rotaÃ§Ã£o de proxies
3. **User Agents**: Alterne user agents para parecer trÃ¡fego natural
4. **Seletores**: Os seletores CSS podem mudar - monitore e atualize
5. **Termos de Uso**: Verifique os termos de uso de cada site
6. **Backup**: FaÃ§a backup do banco de dados regularmente

## ğŸ“ Suporte

- Logs detalhados em `scraper.log`
- Use `--debug` para mais informaÃ§Ãµes
- Verifique a conexÃ£o com Supabase antes de reportar problemas
